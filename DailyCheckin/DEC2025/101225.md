# Daily Check-In - mercredi 10 DEC. 2025

### to-do list
- [x] finish up module 3   
- [ ] module 4
- [x] read 30min
- [ ] run 10min
  
### what I learned today
1. Chi-Square Test (categorical variables) - check my DUNE diary p.117

to check significant association between two categorical variables.\
[explanation](https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL09FalYxZlpCcTV0U2tRbi1DbzktNWcvY2hpLXNxYXVyZS10ZXN0LXYxLm1kP3Q9MTc1MDIzMTIxMSIsInRvb2xfdHlwZSI6Imluc3RydWN0aW9uYWwtbGFiIiwiYXRsYXNfZmlsZV9pZCI6NjExNTQsImFkbWluIjpmYWxzZSwiaWF0IjoxNzU3Njk1NzkwfQ.gxQudjESyrIpQjfQJAQ27szszrWWW6IVRxMlQkYNanE)

2. df.describe(include=['dtype'])
```python
df.describe(include=['object'] # describe on the variables of type 'object'
```

3. [exploratory data analysis](https://github.com/cherry0ung/TIL/blob/main/DailyCheckin/DEC2025/Exploratory_data_analysis_cars.ipynb)

4. [EDA practice w/laptop.csv](https://github.com/cherry0ung/TIL/blob/main/DailyCheckin/DEC2025/parctice_Exploratory_data_analysis.ipynb)

5. Linear Regression : SLR, MLR
- Simple Linear Regression : y = b0 + b1x
```python
from sklearn.linear_model import LinearRegression
lm = LinearRegression() # create LR object
X = df[['highway-mpg']]
Y = df['price]
lm.fit(X, Y) # to fit the model (=find params b0, b1)
Yhat = lm.predict(X)

# check the intercept = b0
lm.intercept_
# check the slope = b1
lm.coef_
```
- Multiple Linear Regression(MLR) : relationship between 1 continuous target Y, multiple predictor Xs. Yhat = b0 + b1x1 + b2x2 + ... + bnxn
```python
Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]
lm.fit(Z, df['price'])
Yhat = lm.predict(Z)

```

6. Polynomial Regression
- useful for describing *curvilinear* relationships
```python
f = np.polyfit(x, y, 3)  # 3rd order
p = np.poly1d(f)
print(p) # ex) -1.557(x1)^3 + 205.8(x2)^2 + 9423x1 + 1.27*10^5
```

7. PREPROCESSING libraries in scikit-learn
```python
from sklearn.preprocessing import PolynomialFeatures
pr = PolynomialFeatures(degree=2, include_bias=False)
x_polly = pr.fit_transform(x[['horsepower', 'curb-weight']])
```
```python
# we can NORMALIZE each feature SIMULTANEOUSLY
from sklearn.preprocessing import StandardScaler
SCALE = StandardScaler()
SCALE.fit(x_data[['horsepower', 'highway-mpg']])
x_scale = SCALE.transform(x_data[['horsepower', 'highway-mpg']])
```

8. Pipelines ??

9. KDE plots = PDF visualisation
~~~python
import numpy as npy
import pandas as pds
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
# Generating Sample Data
npy.random.seed(42)
x = npy.random.rand(100) * 10
y = 3 * x + npy.random.normal(0, 3, 100)  # Linear relation with noise
data = pds.DataFrame({'X': x, 'Y': y})
# Splitting Data
X_train, X_test, y_train, y_test = train_test_split(data[['X']], data['Y'], test_size=0.2, random_state=42)
# Training a Model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Plotting KDE for Observed vs. Predicted Values
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test, label='Actual', fill=True, color='blue')
sns.kdeplot(y_pred, label='Predicted', fill=True, color='red')
plt.xlabel('Target Variable')
plt.ylabel('Density')
plt.title('KDE Plot of Actual vs. Predicted Values')
plt.legend()
plt.show()
~~~

10. 
11. 

### ideas
- function to find variables with strong correlation from correlation table?
- 
